{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Iispar/dl-in-hlt-project/blob/main/course_project_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep learning in Human Language Technology Project (Template)\n",
        "\n",
        "- Student(s) Name(s): Iiro Partanen\n",
        "- Date: 18/10/2023\n",
        "- Chosen Corpus: amazon_reviews_multi\n",
        "- Contributions (if group project): -\n",
        "\n",
        "### Corpus information\n",
        "\n",
        "- Description of the chosen corpus:\n",
        "- Paper(s) and other published materials related to the corpus:\n",
        "- Random baseline performance and expected performance for recent machine learned models:"
      ],
      "metadata": {
        "id": "ucyWlC5gbOyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 1. Setup"
      ],
      "metadata": {
        "id": "D5d-9uxrcDY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -q transformers datasets evaluate\n",
        "!pip install optuna\n",
        "!pip install accelerate -U\n",
        "from transformers import DistilBertTokenizer\n",
        "from transformers import BertTokenizer\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import DataCollatorWithPadding\n",
        "from transformers import DistilBertModel\n",
        "import datasets\n",
        "import sklearn.feature_extraction\n",
        "import torch\n",
        "import transformers\n",
        "import numpy as np\n",
        "import evaluate\n",
        "import optuna"
      ],
      "metadata": {
        "id": "caHHQoqEcG1J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0fb2286-4c06-4191-9ae6-13ac4f94b65b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (3.4.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.12.0)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.21)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.2.4)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (17.0.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dset = 'mteb/amazon_reviews_multi'\n",
        "model = 'distilbert-base-uncased' # base distilbert with cased, because feel like this would fit well with reviews... we will see."
      ],
      "metadata": {
        "id": "CmII7u6vQ6ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 2. Data download, sampling and preprocessing\n",
        "\n",
        "### 2.1. Download the corpus"
      ],
      "metadata": {
        "id": "ovUapilSb8iT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDx40YyzbGPc"
      },
      "outputs": [],
      "source": [
        "engDataset = datasets.load_dataset(dset, name='en'); # imports the dataset.\n",
        "# check it works\n",
        "print(engDataset);"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#FOR TESTING\n",
        "engDataset[\"train\"] = engDataset[\"train\"].select(range(100000))"
      ],
      "metadata": {
        "id": "Gj8PY881OUdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(engDataset)"
      ],
      "metadata": {
        "id": "u3maciqrOZO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Sampling and preprocessing"
      ],
      "metadata": {
        "id": "cXb7CQNCbZOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "engDataset = engDataset.shuffle() # shuffle the dataset for safety.\n",
        "engDataset = engDataset.remove_columns(['id', 'label_text']) # removes everything that we don't need"
      ],
      "metadata": {
        "id": "RO5BXCuRbYKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets look at five results to see if there is more preprocessing to be done\n",
        "\n",
        "print(engDataset['train'][0]['text'])\n",
        "\n",
        "# looks like the title is spaced with \\n\\n, but other than that there is no problems. Looks good to me."
      ],
      "metadata": {
        "id": "tjjitI9YBKZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### tokenization"
      ],
      "metadata": {
        "id": "FVmZJ7xADCQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the BERT tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model) # max len same as what bert uses\n",
        "# Used the BertTokenizer instead of AutoTokenizer, because we want the token type ids to be used for BERT."
      ],
      "metadata": {
        "id": "vKhUoM-1DGVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizes one example\n",
        "def tokenize_example(example):\n",
        "    split = example['text'].split('\\n\\n'); # splits the sentace and title.\n",
        "    return tokenizer.encode_plus(split[0], split[1],\n",
        "             truncation='only_second',\n",
        "             add_special_tokens=True,\n",
        "             max_length=512,\n",
        "             padding='max_length')"
      ],
      "metadata": {
        "id": "6jIjvWvsORvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# map the whole dset\n",
        "\n",
        "eng_tokenized = engDataset.map(tokenize_example)"
      ],
      "metadata": {
        "id": "9L-AumJgT6_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(eng_tokenized['train'][10]) # looks good to me."
      ],
      "metadata": {
        "id": "PhC-ccr3WT5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 3. Machine learning model\n",
        "\n",
        "### 3.1. Model training"
      ],
      "metadata": {
        "id": "F1ntHh_JbrAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# config\n",
        "%%time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Create the BertClassfier class\n",
        "class BertClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BertClassifier, self).__init__()\n",
        "        # hidden size of BERT (always 768), hidden size of our classifier, and number of labels (in this case 5)\n",
        "        D_in, H, D_out = 768, 4, 5\n",
        "\n",
        "        # load the pretrained bert.\n",
        "        self.bert = DistilBertModel.from_pretrained(model)\n",
        "\n",
        "        # basic one layer feed forward network that outputs the labels.\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(D_in, H),\n",
        "            nn.ReLU(),\n",
        "            #nn.Dropout(0.5),\n",
        "            nn.Linear(H, D_out)\n",
        "        )\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        # run the BERT\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "\n",
        "        # Extract the last hidden state of the token for classification\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed tha last hidden state into the classifier. This outputs the labels.\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        if labels is not None:\n",
        "          # calculates the loss.\n",
        "          loss = torch.nn.CrossEntropyLoss();\n",
        "          return (loss(logits,labels),logits);\n",
        "        else:\n",
        "          # if no labels, just return the logits\n",
        "          return (logits,);\n",
        "       # torch.cuda.empty_cache();\n",
        "\n",
        "\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n"
      ],
      "metadata": {
        "id": "Hs2Bf49zbn5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = evaluate.load('accuracy');\n",
        "def compute_accuracy(outputs_and_labels):\n",
        "    outputs, labels = outputs_and_labels;\n",
        "    predictions = np.argmax(outputs, axis=-1); #pick the index of the \"winning\" label\n",
        "    return accuracy.compute(predictions=predictions, references=labels); # calc accuracy"
      ],
      "metadata": {
        "id": "x0i4zPDtcrPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So. The training of this is really slow was something like 1 it/s, with batchsize of 16. This is because the limiter in colab is the memory so lets try to get that lower. So lets freeze all the params we dont want to use that are from the pretrained BERT, this ought to get it a bit higher.\n",
        "\n",
        "This with other changes got it up to about 7 with distilbert and 3.5 with bert which keeps the training possible in colab :)"
      ],
      "metadata": {
        "id": "pyjHQpKnxXVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertClassifier()\n",
        "# get params that we can change\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "#params that are from the bert we should NOT change so lets freeze those.\n",
        "for name, param in model.named_parameters():\n",
        "    if name.startswith('bert'):\n",
        "        param.requires_grad = False\n",
        "\n",
        "# actual changeable params\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "id": "fv-M0iXzf-Mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training params. We optimize these later\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy = 'steps',\n",
        "    logging_strategy = 'steps',\n",
        "    eval_steps = 500,\n",
        "    logging_steps = 500,\n",
        "    learning_rate=1e-4,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    max_steps = 20000,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "  )\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "early_stopping = transformers.EarlyStoppingCallback(5); # stop training if the eval loss is not getting better.\n",
        "\n",
        "# Set the trainer\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    train_dataset = eng_tokenized['train'],\n",
        "    eval_dataset = eng_tokenized['test'],\n",
        "    tokenizer = tokenizer,\n",
        "    data_collator = data_collator,\n",
        "    compute_metrics = compute_accuracy,\n",
        ")\n",
        "\n",
        "# train the model\n",
        "trainer.train()\n",
        ""
      ],
      "metadata": {
        "id": "M-pqjkfvcneI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Hyperparameter optimization"
      ],
      "metadata": {
        "id": "nlO8RVuHcmAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code for hyperparameter optimization here"
      ],
      "metadata": {
        "id": "IzDrTDd0cWOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3. Evaluation on test set"
      ],
      "metadata": {
        "id": "1EzCYTnfcrvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code to evaluate the final model on the test set here"
      ],
      "metadata": {
        "id": "BG7s-yr6crGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4. Multilingual and cross-lingual experiments"
      ],
      "metadata": {
        "id": "zzThmhszBKjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code to train and evaluate the multilingual and cross-lingual models"
      ],
      "metadata": {
        "id": "7YAtSLEEBS91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 4. Results and summary\n",
        "\n",
        "### 4.1 Corpus insights\n",
        "\n",
        "(Briefly discuss what you learned about the corpus and its annotation)\n",
        "\n",
        "### 4.2 Results\n",
        "\n",
        "(Briefly summarize your results)\n",
        "\n",
        "### 4.3 Relation to random baseline / expected performance / state of the art\n",
        "\n",
        "(Compare your results to the random and state-of-the-art performance)\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Bonus Task (optional)\n",
        "\n",
        "### 5.1. Data selection\n",
        "\n",
        "(Briefly describe how many English and target language examples were used and how these were selected, include relevant code)\n",
        "\n",
        "### 5.2 Sentence representations"
      ],
      "metadata": {
        "id": "x7ylOS8FdYZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code to create a sentence embedding for the given text here"
      ],
      "metadata": {
        "id": "32DU04FndRdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3. Cosine similarity"
      ],
      "metadata": {
        "id": "4ghO4JemeFKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code to calculate the cosine similarity of the embeddings and select the target sentence that maximizes the cosine similarity here"
      ],
      "metadata": {
        "id": "9tzYWQ_zeCYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4 Bonus task evaluation\n",
        "\n",
        "(Present the evaluation results here)"
      ],
      "metadata": {
        "id": "8XLZlItdePfJ"
      }
    }
  ]
}