{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Iispar/dl-in-hlt-project/blob/main/course_project_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep learning in Human Language Technology Project (Template)\n",
        "\n",
        "- Student(s) Name(s): Iiro Partanen\n",
        "- Date: 18/10/2023\n",
        "- Chosen Corpus: amazon_reviews_multi\n",
        "- Contributions (if group project): -\n",
        "\n",
        "### Corpus information\n",
        "\n",
        "- Description of the chosen corpus:\n",
        "- Paper(s) and other published materials related to the corpus:\n",
        "- Random baseline performance and expected performance for recent machine learned models:"
      ],
      "metadata": {
        "id": "ucyWlC5gbOyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 1. Setup"
      ],
      "metadata": {
        "id": "D5d-9uxrcDY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -q transformers datasets evaluate\n",
        "!pip install optuna\n",
        "from transformers import BertTokenizer\n",
        "from transformers import AutoTokenizer\n",
        "import datasets\n",
        "import sklearn.feature_extraction\n",
        "import torch\n",
        "import transformers\n",
        "import numpy as np\n",
        "import evaluate\n",
        "import optuna"
      ],
      "metadata": {
        "id": "caHHQoqEcG1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dset = 'mteb/amazon_reviews_multi'\n",
        "model = 'bert-base-cased' # base bert with cased, because feel like this would fit well with reviews... we will see."
      ],
      "metadata": {
        "id": "CmII7u6vQ6ij"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 2. Data download, sampling and preprocessing\n",
        "\n",
        "### 2.1. Download the corpus"
      ],
      "metadata": {
        "id": "ovUapilSb8iT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "PDx40YyzbGPc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d058780a-905c-4a99-bd61-ee43e863d2e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'text', 'label', 'label_text'],\n",
            "        num_rows: 200000\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['id', 'text', 'label', 'label_text'],\n",
            "        num_rows: 5000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['id', 'text', 'label', 'label_text'],\n",
            "        num_rows: 5000\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "engDataset = datasets.load_dataset(dset, name='en'); # imports the dataset.\n",
        "# check it works\n",
        "print(engDataset);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Sampling and preprocessing"
      ],
      "metadata": {
        "id": "cXb7CQNCbZOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "engDataset = engDataset.shuffle() # shuffle the dataset for safety.\n",
        "engDataset = engDataset.remove_columns(['id', 'label_text']) # removes everything that we don't need"
      ],
      "metadata": {
        "id": "RO5BXCuRbYKr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9af56c6d-2203-472c-abd5-d1e78349de36"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'Kept my interest\\n\\nTerrific read. Very scary.', 'label': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets look at five results to see if there is more preprocessing to be done\n",
        "\n",
        "print(engDataset['train'][0]['text'])\n",
        "\n",
        "# looks like the title is spaced with \\n\\n, but other than that there is no problems. Looks good to me."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjjitI9YBKZ0",
        "outputId": "be6db087-0cd7-4133-d55e-7a8c4888787c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kept my interest\n",
            "\n",
            "Terrific read. Very scary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### tokenization"
      ],
      "metadata": {
        "id": "FVmZJ7xADCQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(model) # max len same as what bert uses\n",
        "# Used the BertTokenizer instead of AutoTokenizer, because we want the token type ids to be used for BERT."
      ],
      "metadata": {
        "id": "vKhUoM-1DGVp"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizes one example\n",
        "def tokenize_example(example):\n",
        "    split = example['text'].split('\\n\\n') # splits the sentance and title. So we can insert these into the tokenizer seperatly\n",
        "    return tokenizer.encode_plus(split[0], split[1], # tokenizes the title and review\n",
        "                          truncation='only_second', # cut if the limit of 512 tokens is extended. Cuts always the second sentance so the review.\n",
        "                          add_special_tokens=True, # adds CLS and SEP\n",
        "                          max_length=512, # max len = berts max length, cut rest over this\n",
        "                          padding=True) # pads to max length"
      ],
      "metadata": {
        "id": "6jIjvWvsORvB"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenize_example(engDataset['train'][0])) # check it works."
      ],
      "metadata": {
        "id": "Zvv3IwwBOpOh",
        "outputId": "db147b50-478e-47b5-a790-98e5192bfff5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [101, 26835, 6451, 1139, 2199, 102, 12008, 14791, 21361, 2373, 119, 6424, 13952, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# map the whole dset\n",
        "\n",
        "eng_tokenized = engDataset.map(tokenize_example)"
      ],
      "metadata": {
        "id": "9L-AumJgT6_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(eng_tokenized['train'][10]) # looks good to me."
      ],
      "metadata": {
        "id": "PhC-ccr3WT5R",
        "outputId": "80cb8ecc-5cb1-4957-8b02-604f2d9b13e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'Fits perfectly\\n\\nFits perfectly in my Tucson 2018, only 4 starts due to some marks in parts for lack of protective packing.', 'label': 3, 'input_ids': [101, 17355, 2145, 6150, 102, 17355, 2145, 6150, 1107, 1139, 18740, 1857, 117, 1178, 125, 3816, 1496, 1106, 1199, 6216, 1107, 2192, 1111, 2960, 1104, 9760, 16360, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 3. Machine learning model\n",
        "\n",
        "### 3.1. Model training"
      ],
      "metadata": {
        "id": "F1ntHh_JbrAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code to train the transformer based model on the training set and evaluate the performance on the validation set here"
      ],
      "metadata": {
        "id": "Hs2Bf49zbn5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Hyperparameter optimization"
      ],
      "metadata": {
        "id": "nlO8RVuHcmAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code for hyperparameter optimization here"
      ],
      "metadata": {
        "id": "IzDrTDd0cWOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3. Evaluation on test set"
      ],
      "metadata": {
        "id": "1EzCYTnfcrvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code to evaluate the final model on the test set here"
      ],
      "metadata": {
        "id": "BG7s-yr6crGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4. Multilingual and cross-lingual experiments"
      ],
      "metadata": {
        "id": "zzThmhszBKjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code to train and evaluate the multilingual and cross-lingual models"
      ],
      "metadata": {
        "id": "7YAtSLEEBS91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 4. Results and summary\n",
        "\n",
        "### 4.1 Corpus insights\n",
        "\n",
        "(Briefly discuss what you learned about the corpus and its annotation)\n",
        "\n",
        "### 4.2 Results\n",
        "\n",
        "(Briefly summarize your results)\n",
        "\n",
        "### 4.3 Relation to random baseline / expected performance / state of the art\n",
        "\n",
        "(Compare your results to the random and state-of-the-art performance)\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Bonus Task (optional)\n",
        "\n",
        "### 5.1. Data selection\n",
        "\n",
        "(Briefly describe how many English and target language examples were used and how these were selected, include relevant code)\n",
        "\n",
        "### 5.2 Sentence representations"
      ],
      "metadata": {
        "id": "x7ylOS8FdYZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code to create a sentence embedding for the given text here"
      ],
      "metadata": {
        "id": "32DU04FndRdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3. Cosine similarity"
      ],
      "metadata": {
        "id": "4ghO4JemeFKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code to calculate the cosine similarity of the embeddings and select the target sentence that maximizes the cosine similarity here"
      ],
      "metadata": {
        "id": "9tzYWQ_zeCYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4 Bonus task evaluation\n",
        "\n",
        "(Present the evaluation results here)"
      ],
      "metadata": {
        "id": "8XLZlItdePfJ"
      }
    }
  ]
}